# -*- coding: utf-8 -*-
"""customer_profitability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pVE3vhZYkCCroLZiQNflHUbJ2e7kW0w6

<center>

![](https://drive.google.com/uc?export=view&id=1R6EAVE0Ysym-UNtZR-Ze_HcqQLDmH0bd)
<p>
College: Engineering and Information Technology <br>
Department: Information Technology <br>


Buisness Intillegence - INS402 <br>



**ustomer profitability of superstore analysis** <br>
-

**Prepared by :**
- 202111148 - Asmaa Luai Abdulla
- 202110929 - Lujain Adnan Thouqan
-202111564 - Ahmad Jamal Shihab
-202110946 - Sara Al najjar
-20192024 - Rayan hassan
-202110248 - Maitha Sultan Alremeithi

**Supervised by:**

 **Dr .Ghazi AlNaymat**

Academic semester: Fall 2024 - 2025 <br>

<br>
</p></center>
</p></center>

# **Data Description**

The dataset titled "Sales Forecasting" is a valuable repository of data, containing several important attributes that offer valuable insights into sales patterns.

The dataset seems to provide insights into sales transactions, customer details, and product information

Let's discover the fundamental columns, understand their significance, and grasp the types of data they hold.

Order ID: A unique identifier for each order placed.

Order Date: The date when the order was placed.

Ship Date: The date when the ordered items were shipped.

Ship Mode: The shipping mode chosen for the order (e.g., Second Class).

Customer ID: A unique identifier for each customer.

Customer Name: The name of the customer placing the order.

Segment: The market segment to which the customer belongs (e.g., Consumer, Corporate)

Country: The country where the order was placed (e.g., United States).

City: The city where the order was placed.

State: The state within the country where the order was placed.

Postal Code: The postal code associated with the order location.

Region: The region of the country where the order was placed (e.g., South, West).

Product ID: A unique identifier for each product.

Category: The broad category to which the product belongs (e.g., Furniture, Office Supplies).

Sub-Category: The specific sub-category to which the product belongs (e.g., Bookcases, Chairs)

Product Name: The name of the product ordered.

Sales: The sales amount associated with the order.

# **Data understanding and preprocessing**
"""

!pip install pandas
!pip install plotly
!pip install matplotlib
!pip install scikit-learn
!pip install seaborn
!pip install numpy
!pip install gradio

!pip install viola

import pandas as pd

# df = pd.read_csv('/content/drive/MyDrive/Sample - Superstore (3).csv', encoding='latin1')
df = pd.read_csv('Superstore.csv', encoding='latin1')
df.head()

df.describe()

df.describe(include='O')

df.columns

#df.dtypes
df.info()

print("Shipping modes: ")
print(df['ShipMode'].unique())
print("Countries: ")
print(df['Country'].unique())
print('Number of states: ' + str(df['State'].unique().shape[0]))
print("Region: ")
print(df['Region'].unique())
print('Number of cities: ' + str(df['City'].unique().shape[0]))
print("Categories: ")
print(df['Category'].unique())
print("Number of Sub categories: " + str(df['Sub-Category'].unique().shape[0]))
print("Number of Products: " + str(df['Product ID'].unique().shape[0]))

df = df.drop(['Row ID','Customer ID','Product ID','Country','Postal Code'], axis=1)

df

df['ShipDate'] = pd.to_datetime(df['ShipDate'])
df['OrderDate'] = pd.to_datetime(df['OrderDate'])

df['Ship_month_added']=df['ShipDate'].dt.month
df['Ship_day_added']=df['ShipDate'].dt.day
df['Ship_year_added'] = df['ShipDate'].dt.year

df['Order_month_added']=df['OrderDate'].dt.month
df['Order_day_added']=df['OrderDate'].dt.day
df['Order_year_added'] = df['OrderDate'].dt.year

df['Delivery Days'] = df['ShipDate'] - df['OrderDate']

df['Delivery Days'].describe()

#to get the days only
df['Delivery Days']=df['Delivery Days'].dt.days
df['Delivery Days'].describe()

df

# check missing values
print(df.isnull().sum())

#The duplicated Rows
df[df.duplicated(keep=False)]

df.drop_duplicates(inplace=True)
print("Removed Duplicates :")
print(df.duplicated().sum())

"""# **Data visualization**

# Feature Analysis
"""

#!pip install plotly
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Plot distribution of the Numerical columns
sales = go.Box(x=df['Sales'],name='Sales')
Quantity = go.Box(x=df['Quantity'],name='Quantity')
Discount = go.Box(x=df['Discount'],name='Discount')
Profit = go.Box(x=df['Profit'],name='Profit')
Delivery_days = go.Box(x=df['Delivery Days'],name='Delivery Days')


fig = make_subplots(rows=4, cols=2)
fig.append_trace(sales, row = 1, col = 1)
fig.append_trace(Quantity, row = 1, col = 2)
fig.append_trace(Discount, row = 2, col = 1)
fig.append_trace(Profit, row = 2, col = 2)
fig.append_trace(Delivery_days, row = 3, col = 1)
fig.update_layout(
    title_text = 'Distribution of the numerical data',
    title_font_size = 24,
    title_x=0.45)

fig.show()

# Plot distribution of the Categorial columns
ship_mode = go.Histogram(x=df["ShipMode"],name="Ship Mode")
segment = go.Histogram(x=df["Segment"],name="Segment")
Region = go.Histogram(x=df["Region"],name="Region")
category = go.Histogram(x=df["Category"],name="Category")

fig = make_subplots(rows=2, cols=2)
fig.append_trace(ship_mode, row = 1, col = 1)
fig.append_trace(segment, row = 1, col = 2)
fig.append_trace(Region, row = 2, col = 1)
fig.append_trace(category, row = 2, col = 2)
fig.update_layout(
    title_text = 'Number of occurrences of each category in the categorical variables',
    title_x=0.45
)

fig.show()

import plotly.express as px # Import plotly.express
state_ship_mode_counts = df.groupby(['State', 'ShipMode']).size().reset_index(name='Count')
fig = px.treemap(state_ship_mode_counts, path=['State', 'ShipMode'], values='Count')
fig.update_layout( title='Treemap of State vs. Ship Mode',title_x=0.45,width=1600,title_font=dict(size=24, family="Arial", color="black"))
fig.show()

# What is the average order value for each shipping mode?
for i in df['ShipMode'].unique():
    avarage = df[df['ShipMode']==i]['Sales'].mean()
    print(60*'-')
    print(f'Avarage order value for "{i}" shipping mode is {avarage:.2f}')

# What is the average sales value for each sub-category product?
for i in df['Sub-Category'].unique():
    avarage = df[df['Sub-Category']==i]['Sales'].mean()
    print(60*'-')
    print(f'Avarage order value for "{i}" shipping mode is {avarage:.2f}')

# How does the average order value for each customer segment?
for i in df['Segment'].unique():
    avarage = df[df['Segment']==i]['Sales'].mean()
    print(60*'-')
    print(f'Avarage sale for {i} is {avarage:.2f}')

"""# **What is the relation between the region and shipment mode?**"""

import matplotlib.pyplot as plt # import the matplotlib library with the alias 'plt'
import seaborn as sns

df

#Unknown error with hue = 'ShipMode' Andrew T
# plt.figure(figsize=(15,6))
# sns.countplot(df, x='Region', hue='ShipMode', palette='coolwarm') #Paired #muted
# plt.title('Count of Ship Modes by Region')
# plt.show()

monthly_sales = df.groupby(['Order_year_added', 'Order_month_added'])[['Profit', 'Sales']].sum().reset_index()
monthly_sales

"""# Profit accross the state"""

state_df= df.groupby(['State', 'Region'], as_index=False)['Profit'].sum()
state_df

state_sales = df.groupby('State')['Profit'].sum().reset_index()

# Find the state with the highest sales
state_with_highest_sales = state_sales[state_sales['Profit'] == state_sales['Profit'].max()]

print("State with the highest profit:", state_with_highest_sales['State'].values[0])
print("Total sales in that profit:", state_with_highest_sales['Profit'].values[0])
print(f"with total of {round(state_with_highest_sales['Profit'].values[0] / (df['Profit'].sum()) * 100, 2)}% of sales")

"""# Does this variance of delivery days affect the sales?

"""

#Andrew T. - hidden because this does not show in voila
# scatter_fig = px.scatter(df, x='Delivery Days', y='Profit', color='ShipMode',
#                          title='Impact of Delivery Days Variance on Profit',
#                          labels={'Delivery Days Variance': 'Delivery Days Variance', 'Profit': 'Profit'})
# scatter_fig .update_layout(title="Impact of Delivery Days Variance on Profit",title_x=0.45,width=1300,title_font=dict(size=24, family="Arial", color="black"))
# scatter_fig.show()

"""no ,they are independent"""

#sorting profit of sub_category
sub_category = pd.DataFrame(df.groupby('Sub-Category')['Profit'].sum().sort_values(ascending =False))
p = pd.DataFrame(sub_category.sort_values('Profit',ascending=False))
p

#checking profit of segment
segment = pd.DataFrame(df.groupby('Segment')['Profit'].sum())
segment

#profit/ sales by region
region = df.groupby(['Region'])[['Sales', 'Profit']].sum().sort_values(by='Sales')
region

#adding sales by category
category = df.groupby(['Category'])[['Sales', 'Profit', 'Quantity']].sum()
category

customer_order_counts = df.groupby('Customer Name')['Order ID'].nunique()
frequents = customer_order_counts[customer_order_counts > 10]
num_frequents = len(frequents)
print("Number of frequent customers (with more than 10 orders):", num_frequents)

nonfrequent=customer_order_counts[customer_order_counts <= 10]
num_nonfrequents=len(nonfrequent)

plt.figure(figsize=(6, 6))
plt.pie([num_frequents, num_nonfrequents], labels=['Frequent', 'Not Frequent'], autopct='%1.1f%%', colors=['#8cb5db', '#8c8c8c'])
plt.title('Relationship Between Frequent and Not Frequent Clients')
plt.show()

import pandas as pd

# Assuming your DataFrame is named 'df'
# Example for categorizing based on profit
high_threshold = df['Profit'].quantile(0.80)
low_threshold = df['Profit'].quantile(0.20)

# Create a new column 'Customer_Category'
df['Customer_Category'] = pd.cut(df['Profit'],
                                 bins=[-float('inf'), low_threshold, high_threshold, float('inf')],
                                 labels=['Low value', 'Moderate value', 'High value'])

# Display the number of customers in each category
print(df['Customer_Category'].value_counts())

df[['Customer Name', 'Profit', 'Customer_Category']].head()

df.to_csv('preprocessed_data.csv', index=False)

df

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, accuracy_score

# Assuming the dataset is loaded into 'df' (your DataFrame)

# Drop columns that are not useful (e.g., 'Order ID', 'Customer Name')
# df = df.drop(columns=['OrderID', 'CustomerName']) #Removed: Andrew T.

# Handle missing values (if any)
df = df.dropna()


# Drop original datetime columns after conversion
df = df.drop(columns=['OrderDate', 'ShipDate'])

# Handle categorical variables
# List of categorical columns for label encoding (ordinal)
categorical_columns_label = ['ShipMode', 'Segment', 'Region', 'Ship_month_added',
                             'Ship_day_added', 'Ship_year_added', 'Order_month_added',
                             'Order_day_added', 'Order_year_added','Sub-Category','Product Name']

# Apply Label Encoding for ordinal categories
label_encoder = LabelEncoder()
for col in categorical_columns_label:
    df[col] = label_encoder.fit_transform(df[col])

# List of categorical columns for one-hot encoding (nominal)
categorical_columns_onehot = ['Category', 'State', 'City']

# One-Hot Encoding for nominal categories
df = pd.get_dummies(df, columns=categorical_columns_onehot, drop_first=True)

# Target variable (Customer Category) and Features (X)
X = df.drop(columns=['Customer_Category','Profit'])  # Features
y = df['Customer_Category']  # Target

# Check for non-numeric columns before scaling (to debug)
print(X.dtypes)  # Print data types of columns in X to ensure all are numeric

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ensure all columns in X_train are numeric
if not all([pd.api.types.is_numeric_dtype(X_train[col]) for col in X_train.columns]):
    raise ValueError("There are still non-numeric columns in the dataset!")

# Standardize the features (important for gradient boosting and other models)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform the training data
X_test_scaled = scaler.transform(X_test)  # Transform the test data

# Model training (using GradientBoostingClassifier)
model = GradientBoostingClassifier(n_estimators=200, max_depth=7, learning_rate=0.05)
model.fit(X_train_scaled, y_train)

# Predict the categories for the test set
y_pred = model.predict(X_test_scaled)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# If you'd like to check the confusion matrix for a deeper evaluation (optional)
from sklearn.metrics import confusion_matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Import necessary libraries for visualization
import matplotlib.pyplot as plt
import numpy as np

# Track accuracy improvement over iterations
train_accuracies = []
test_accuracies = []

# Create a Gradient Boosting model with staged predictions to monitor performance
model = GradientBoostingClassifier(n_estimators=200, max_depth=7, learning_rate=0.05)
model.fit(X_train_scaled, y_train)

# Use staged_predict to see performance improvement
for y_train_pred, y_test_pred in zip(model.staged_predict(X_train_scaled), model.staged_predict(X_test_scaled)):
    # Calculate accuracy for each iteration
    train_accuracies.append(accuracy_score(y_train, y_train_pred))
    test_accuracies.append(accuracy_score(y_test, y_test_pred))

# Plotting the improvement in accuracy
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy', color='blue', marker='o')
plt.plot(range(1, len(test_accuracies) + 1), test_accuracies, label='Test Accuracy', color='green', marker='x')
plt.title('Training vs Test Accuracy Improvement Over Iterations')
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# Import necessary libraries for plotting
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Predict the categories for the test set
y_pred = model.predict(X_test_scaled)

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Get feature importance from the model
feature_importance = model.feature_importances_
sorted_idx = np.argsort(feature_importance)[::-1]  # Sort in descending order
important_features = X.columns[sorted_idx]

# Plot the feature importance
plt.figure(figsize=(10, 6))
plt.barh(important_features[:10][::-1], feature_importance[sorted_idx][:10][::-1])
plt.title("Top 10 Important Features")
plt.xlabel("Feature Importance Score")
plt.show()

from sklearn.preprocessing import LabelEncoder
import joblib

label_encoders = {} #Added Andrew T.

# Retrain the encoder on original string values for 'Region'
label_encoder_region = LabelEncoder()
original_categories = ['West', 'East', 'North', 'South']  # Example, replace with your actual categories
label_encoder_region.fit(original_categories)

label_encoders['Region'] = label_encoder_region

# Save the corrected encoder for future use
joblib.dump(label_encoder_region, 'label_encoder_region.pkl')

label_encoders['Region'] = joblib.load('label_encoder_region.pkl') #not defined Andrew T.

print(label_encoders['Region'].classes_) #not defined Andrew T.
# Expected output: ['East', 'North', 'South', 'West']

def predict_customer_category(discount, sales, region, segment):
    try:
        # Ensure inputs are transformed using properly trained encoders
        region_encoded = label_encoders['Region'].transform([region])[0]
        segment_encoded = label_encoders['Segment'].transform([segment])[0]

        # Prepare the input data
        input_data = pd.DataFrame({
            'Discount': [discount],
            'Sales': [sales],
            'Region': [region_encoded],
            'Segment': [segment_encoded]
        })

        # Standardize the input data
        input_data_scaled = scaler.transform(input_data)

        # Predict the customer category
        prediction = model.predict(input_data_scaled)
        return f"Predicted Customer Category: {prediction[0]}"

    except ValueError as e:
        return f"Input Error: {e}"
    except Exception as e:
        return f"Error in prediction: {e}"

print(label_encoders['Region'].classes_)  # ['East', 'North', 'South', 'West']

# Retrain the encoder for 'Segment'
label_encoder_segment = LabelEncoder()
original_segments = ['Consumer', 'Corporate', 'Home Office']  # Replace with actual unique categories
label_encoder_segment.fit(original_segments)

# Save the corrected encoder for future use
joblib.dump(label_encoder_segment, 'label_encoder_segment.pkl')

label_encoders['Segment'] = joblib.load('label_encoder_segment.pkl')

def predict_customer_category(discount, sales, region, segment):
    try:
        # Ensure inputs are transformed using properly trained encoders
        region_encoded = label_encoders['Region'].transform([region])[0]
        segment_encoded = label_encoders['Segment'].transform([segment])[0]

        # Prepare the input data
        input_data = pd.DataFrame({
            'Discount': [discount],
            'Sales': [sales],
            'Region': [region_encoded],
            'Segment': [segment_encoded]
        })

        # Standardize the input data
        input_data_scaled = scaler.transform(input_data)

        # Predict the customer category
        prediction = model.predict(input_data_scaled)
        return f"Predicted Customer Category: {prediction[0]}"

    except ValueError as e:
        return f"Input Error: {e}"
    except Exception as e:
        return f"Error in prediction: {e}"

print(label_encoders['Region'].classes_)  # ['East', 'North', 'South', 'West']
print(label_encoders['Segment'].classes_)  # ['Consumer', 'Corporate', 'Home Office']

print(type(scaler))  # Expected: <class 'sklearn.preprocessing._data.StandardScaler'>

from sklearn.preprocessing import StandardScaler

# Ensure the scaler is trained on the training data (X_train_scaled already created earlier)
scaler = StandardScaler()
scaler.fit(X_train)  # Use the original training data for fitting

import joblib

# Save the scaler for future use
joblib.dump(scaler, 'scaler.pkl')

scaler = joblib.load('scaler.pkl')

def predict_customer_category(discount, sales, region, segment):
    try:
        # Ensure inputs are transformed using properly trained encoders
        region_encoded = label_encoders['Region'].transform([region])[0]
        segment_encoded = label_encoders['Segment'].transform([segment])[0]

        # Prepare the input data
        input_data = pd.DataFrame({
            'Discount': [discount],
            'Sales': [sales],
            'Region': [region_encoded],
            'Segment': [segment_encoded]
        })

        # Standardize the input data using the scaler
        input_data_scaled = scaler.transform(input_data)

        # Predict the customer category
        prediction = model.predict(input_data_scaled)
        return f"Predicted Customer Category: {prediction[0]}"

    except ValueError as e:
        return f"Input Error: {e}"
    except Exception as e:
        return f"Error in prediction: {e}"

expected_columns = X_train.columns
print("Expected feature names:", expected_columns)

def predict_customer_category(discount, sales, region, segment):
    try:
        # Prepare input data
        input_data = prepare_input_data(discount, sales, region, segment)

        # Standardize the input data
        input_data_scaled = scaler.transform(input_data)

        # Predict the customer category
        prediction = model.predict(input_data_scaled)
        return f"Predicted Customer Category: {prediction[0]}"

    except ValueError as e:
        return f"Input Error: {e}"
    except Exception as e:
        return f"Error in prediction: {e}"

def prepare_input_data(discount, sales, region, segment):
    # Encode categorical variables
    region_encoded = label_encoders['Region'].transform([region])[0]
    segment_encoded = label_encoders['Segment'].transform([segment])[0]

    # Create base input dictionary
    input_data = {
        'Discount': [discount],
        'Sales': [sales],
        'Region': [region_encoded],
        'Segment': [segment_encoded]
    }

    # Add missing columns with zeros using a dictionary comprehension
    input_data_full = {col: input_data.get(col, [0]) for col in expected_columns}

    # Create DataFrame from the dictionary
    input_df = pd.DataFrame(input_data_full)

    return input_df

# Example input
test_input = prepare_input_data(0.2, 300, 'South', 'Consumer')

# Standardize the input data
test_input_scaled = scaler.transform(test_input)

print("Scaled input data:", test_input_scaled)

prediction = model.predict(test_input_scaled)
print("Predicted Category:", prediction)

#Error with markup safe version compatibility - need fix Andrew T.

# import gradio as gr

# def predict_customer_category(discount, sales, region, segment):
#     try:
#         input_data = prepare_input_data(discount, sales, region, segment)
#         input_data_scaled = scaler.transform(input_data)
#         prediction = model.predict(input_data_scaled)
#         return f"The predicted customer category is: {prediction[0]}"
#     except Exception as e:
#         return f"Error in prediction: {str(e)}"

# # Fix: Convert classes_ to a list
# region_choices = list(label_encoders['Region'].classes_)
# segment_choices = list(label_encoders['Segment'].classes_)

# interface = gr.Interface(
#     fn=predict_customer_category,
#     inputs=[
#         gr.Number(label="Discount"),
#         gr.Number(label="Sales"),
#         gr.Dropdown(label="Region", choices=region_choices),
#         gr.Dropdown(label="Segment", choices=segment_choices)
#     ],
#     outputs="text"
# )

# interface.launch()

import pandas as pd
from sklearn.metrics import accuracy_score, classification_report

# Assuming you already have your model and test set (y_test and y_pred)

# Predict the categories for the test set
y_pred = model.predict(X_test_scaled)

# Create a DataFrame to compare actual and predicted categories
comparison_df = pd.DataFrame({
    'Actual Category': y_test,
    'Predicted Category': y_pred
})

# If you want to add a "Correct" column to check if predictions are correct
comparison_df['Correct'] = comparison_df['Actual Category'] == comparison_df['Predicted Category']

# Print the comparison table
comparison_df # Show first few rows of the comparison

# If you want to calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")


# If you'd like to save the comparison to a CSV file (optional)
comparison_df.to_csv('category_comparison.csv', index=False)

#Added by Andrew T. and Sharron T. , Temple University

#conda install -c conda-forge voila
#voila customer_profitability.ipynb
#Run without code
#voila customer_profitability.ipynb --no-input